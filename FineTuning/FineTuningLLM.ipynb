{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concepts Related to Fine tuning\n",
    "\n",
    "* ### Pre trained Model\n",
    "* ### Transfer Learning\n",
    "* ### Fine Tuning\n",
    "\n",
    "# Pre trained model\n",
    "Pretrained models are Machine learning or Neural networks those are trained on a huge corpus of data usually to accompolish a task such as Text generation, Image recognition or Object detection. The training data could be a text, numbers or image data\n",
    "\n",
    "eg : LeNet-5, NVIDIA StyleGAN, GPT-Models, NVIDIA NeMo Megatron, BERT, Alexnet\n",
    "\n",
    "1. It helps reduce overall training time\n",
    "2. It reduces resources required for solving new problems\n",
    "3. It also produces remarkable results with less number of specific training sample\n",
    "\n",
    "# Transfer Learning\n",
    "In Deep Learning context, Transfer learning refers to application of learnings in one problem to solve another problem. A neural network trained on one dataset is used as a base to solve a related problem.\n",
    "For example A neural network trained on detection of objects (such as OpenCV Or Detectron2 ) can be used to identify whether or not a rider is wearing a helmet.\n",
    "\n",
    "Techniques\n",
    "1. Train the model on related data and then retrain wholly or using the layers of desired problem\n",
    "2. Using pre-trained model and then retrain wholly or using the layers of desired problem\n",
    "3. Using Feature extraction technique (also called as representation learning), to extract the important features of the data and then use the same for a specific problem\n",
    "\n",
    "\n",
    "# Fine Tuning\n",
    "The process of fine tuning refers to that of adjusting the weights of a machine learning /deep learning model to obtain more accurate results with a given dataset.                                         Fine tuning makes use of smaller labeled dataset to fine-tune a pre-trained model.\n",
    "\n",
    "Approaches\n",
    "1. Update weights in new dense layers specifically added to existing model OR\n",
    "2. Update weights in all the layers of the existing model.\n",
    "3. Use output of the model to train another classifier model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder - Decoder Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "X1 --> B[Encoder]\n",
    "X2 --> B\n",
    "X3 --> B\n",
    "X4 --> B\n",
    "B --> |Important part of the input|C[Fixed Length Representation]\n",
    "C --> |Output Representation|D[Decoder]\n",
    "D --> Y1\n",
    "D --> Y2\n",
    "D --> Y3\n",
    "D --> Y4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Model\n",
    " the encoder and decoder are trained together, we get sequence to sequence models. They can also be created using CNN/RNN/LSTM networks.\n",
    "\n",
    "#### Example  : \n",
    "Encoder: RNN/LSTM Input :Text in Spanish LanguageExample \n",
    "Decoder: RNN/LSTM Output :Text in English Language\n",
    "            \n",
    "Encoder: Decoder: RNN/LSTM \n",
    "Output : CNN Input:ImageImage Caption\n",
    "\n",
    "##### Transformer architectures can have both encoder and decoder components, enabling them to handle sequence to sequence tasks. Examples of such architectures include T5 and BART. Note that sequence to sequence models convert entire input sequence into a target sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Regressive Models\n",
    "In Natural Language Generation (NLG) scenarios such as generating a blog on a given topic, when the entire text sequence is not known, sequence to sequence models cannot be used. Auto-regressive models are used in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Encoding Model\n",
    "Auto Encoding models learn to reconstruct the data from a corrupted input (in Fill in the blanks Fashion)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
