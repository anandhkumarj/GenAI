{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ab170a",
   "metadata": {},
   "source": [
    "Reference Link : https://huggingface.co/t5-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902cb56",
   "metadata": {},
   "source": [
    "Hugging Face T5 Documentation: <br>\n",
    "    https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85155679",
   "metadata": {},
   "source": [
    "<b>Problem Statement </b>: Fine tune a model to identify the product given a product review.<br>\n",
    "<b>Data set</b> https://data.world/datafiniti/consumer-reviews-of-amazon-products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1286105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "     -------------------------------------- 486.2/486.2 kB 7.7 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 24.1 MB/s eta 0:00:00\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.0/132.0 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-win_amd64.whl (21.5 MB)\n",
      "     --------------------------------------- 21.5/21.5 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "     ------------------------------------- 163.8/163.8 kB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from datasets) (1.2.4)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.1/77.1 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "     ---------------------------------------- 3.5/3.5 MB 24.5 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "     ------------------------------------- 236.8/236.8 kB 14.2 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp38-cp38-win_amd64.whl (263 kB)\n",
      "     -------------------------------------- 263.9/263.9 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rouge-score) (1.0.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=703c0b3a14075c03908da6f814076b28054bcb4e9b4f102ff66f55f2fcf7ccec\n",
      "  Stored in directory: c:\\users\\supriya.aras\\appdata\\local\\pip\\cache\\wheels\\24\\55\\6f\\ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: tokenizers, safetensors, xxhash, tqdm, pyarrow, fsspec, dill, multiprocess, huggingface-hub, transformers, rouge-score, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.3\n",
      "    Uninstalling huggingface-hub-0.13.3:\n",
      "      Successfully uninstalled huggingface-hub-0.13.3\n",
      "Successfully installed datasets-2.13.1 dill-0.3.6 fsspec-2023.6.0 huggingface-hub-0.15.1 multiprocess-0.70.14 pyarrow-12.0.1 rouge-score-0.1.2 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2 xxhash-3.2.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 3.3.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets transformers rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c15d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba0f3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>prod_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solid entry level Kindle. Great for kids. Gift...</td>\n",
       "      <td>Product:Kindle , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This make an excellent ebook reader. Don't exp...</td>\n",
       "      <td>Product:Ebook Reader , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I use this every day on my commute. Great batt...</td>\n",
       "      <td>Product:Unknown , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Everything is great except that i can't read e...</td>\n",
       "      <td>Product:Kindle Books , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I never used kindle before. My wife gifted me ...</td>\n",
       "      <td>Product:Kindle\\n , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Solid entry level Kindle. Great for kids. Gift...   \n",
       "1  This make an excellent ebook reader. Don't exp...   \n",
       "2  I use this every day on my commute. Great batt...   \n",
       "3  Everything is great except that i can't read e...   \n",
       "4  I never used kindle before. My wife gifted me ...   \n",
       "\n",
       "                              prod_sentiment  \n",
       "0        Product:Kindle , Sentiment:Positive  \n",
       "1  Product:Ebook Reader , Sentiment:Positive  \n",
       "2       Product:Unknown , Sentiment:Positive  \n",
       "3  Product:Kindle Books , Sentiment:Positive  \n",
       "4      Product:Kindle\\n , Sentiment:Positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path  = \"D:/Sups/Python/OpenAI/T5/review_data_Product.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5318c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>prod_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solid entry level Kindle. Great for kids. Gift...</td>\n",
       "      <td>Product:Kindle , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This make an excellent ebook reader. Don't exp...</td>\n",
       "      <td>Product:Ebook Reader , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I use this every day on my commute. Great batt...</td>\n",
       "      <td>Product:Unknown , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Everything is great except that i can't read e...</td>\n",
       "      <td>Product:Kindle Books , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I never used kindle before. My wife gifted me ...</td>\n",
       "      <td>Product:Kindle\\n , Sentiment:Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Solid entry level Kindle. Great for kids. Gift...   \n",
       "1  This make an excellent ebook reader. Don't exp...   \n",
       "2  I use this every day on my commute. Great batt...   \n",
       "3  Everything is great except that i can't read e...   \n",
       "4  I never used kindle before. My wife gifted me ...   \n",
       "\n",
       "                              prod_sentiment  \n",
       "0        Product:Kindle , Sentiment:Positive  \n",
       "1  Product:Ebook Reader , Sentiment:Positive  \n",
       "2       Product:Unknown , Sentiment:Positive  \n",
       "3  Product:Kindle Books , Sentiment:Positive  \n",
       "4      Product:Kindle\\n , Sentiment:Positive  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537c45b",
   "metadata": {},
   "source": [
    "We will use one of the T5 models to identify the product, sentiment and main concern area of the cusomer from the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a3ccbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\supriya.aras\\appdata\\roaming\\python\\python38\\site-packages (from torch) (2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich[jupyter] in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (13.4.2)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rich[jupyter]) (4.5.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rich[jupyter]) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rich[jupyter]) (2.2.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from rich[jupyter]) (7.6.3)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (7.22.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.0.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.3.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]) (5.1.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich[jupyter]) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.0.17)\n",
      "Requirement already satisfied: backcall in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (5.0.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (62.4.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.17.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.3.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.15.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.17.3)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (6.0.7)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.10.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (20.0.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (227)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.14.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.1.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.8.4)\n",
      "Requirement already satisfied: testpath in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.4.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (3.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.20)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.5.1)\n",
      "Requirement already satisfied: async-generator in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (1.10)\n",
      "Requirement already satisfied: packaging in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (20.9)\n",
      "Requirement already satisfied: webencodings in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (0.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\supriya.aras\\anaconda3\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<9,>=7.5.1->rich[jupyter]) (2.4.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install rich[jupyter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be00d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85dd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the T5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e34ddd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "404670fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e471c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductExtractor(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataframe, tokenizer, source_len, target_len, source_text, target_text\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            source_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.source_len = source_len\n",
    "        self.summ_len = target_len\n",
    "        self.target_text = self.data[target_text]\n",
    "        self.source_text = self.data[source_text]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.target_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        # cleaning data so as to ensure data is in string type\n",
    "        source_text = \" \".join(source_text.split())\n",
    "        target_text = \" \".join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text],\n",
    "            max_length=self.source_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text],\n",
    "            max_length=self.summ_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        source_mask = source[\"attention_mask\"].squeeze()\n",
    "        target_ids = target[\"input_ids\"].squeeze()\n",
    "        target_mask = target[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\n",
    "            \"source_ids\": source_ids.to(dtype=torch.long),\n",
    "            \"source_mask\": source_mask.to(dtype=torch.long),\n",
    "            \"target_ids\": target_ids.to(dtype=torch.long),\n",
    "            \"target_ids_y\": target_ids.to(dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc0de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 10 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "026c1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          if _%10==0:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67eff45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T5Trainer(\n",
    "    dataframe, source_text, target_text, model_params, output_dir=\"./review_outputs/\"\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text, target_text]]\n",
    "    display_df(dataframe.head(2))\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation.\n",
    "    train_size = 0.8\n",
    "    train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"])\n",
    "    val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = ProductExtractor(\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "    val_set = ProductExtractor(\n",
    "        val_dataset,\n",
    "        tokenizer,\n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"],\n",
    "        source_text,\n",
    "        target_text,\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(), lr=model_params[\"LEARNING_RATE\"]\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "    console.log(f\"[Saving Model]...\\n\")\n",
    "    # Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "    # evaluating test dataset\n",
    "    console.log(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
    "        final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir, \"predictions_prod_senti.csv\"))\n",
    "\n",
    "    console.save_text(os.path.join(output_dir, \"review_logs.txt\"))\n",
    "\n",
    "    console.log(f\"[Validation Completed.]\\n\")\n",
    "    console.print(\n",
    "        f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"review_model_files\")}\\n\"\"\"\n",
    "    )\n",
    "    console.print(\n",
    "        f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'review_predictions.csv')}\\n\"\"\"\n",
    "    )\n",
    "    console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'review_logs.txt')}\\n\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a3b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's define model parameters specific to T5\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-base\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 3,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 50,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8d9486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rich: for a better display on terminal\n",
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "# define a rich console logger\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9fe53a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:29:20] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-base<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:16</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:29:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-base\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m16\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\supriya.aras\\Anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[17:29:27] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:27</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[17:29:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                               \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m27\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">                             source_text                              </span>|<span style=\"font-weight: bold\">                target_text               </span>|\n",
       "|----------------------------------------------------------------------+------------------------------------------|\n",
       "| Extract Product and Sentiment: Solid entry level Kindle. Great for   |    Product:Kindle , Sentiment:Positive   |\n",
       "| kids. Gifted for a kid of my friend and they love to use it to read  |                                          |\n",
       "|                       more than their iPads.                         |                                          |\n",
       "| Extract Product and Sentiment: This make an excellent ebook reader.  | Product:Ebook Reader , Sentiment:Positive|\n",
       "| Don't expect much from this device except to read basic ebooks. The  |                                          |\n",
       "|        good thing is it's cheap and good to read in the sun.         |                                          |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n",
       "|\u001b[1m                             source_text                             \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m               target_text               \u001b[0m|\n",
       "|----------------------------------------------------------------------+------------------------------------------|\n",
       "| Extract Product and Sentiment: Solid entry level Kindle. Great for   |    Product:Kindle , Sentiment:Positive   |\n",
       "| kids. Gifted for a kid of my friend and they love to use it to read  |                                          |\n",
       "|                       more than their iPads.                         |                                          |\n",
       "| Extract Product and Sentiment: This make an excellent ebook reader.  | Product:Ebook Reader , Sentiment:Positive|\n",
       "| Don't expect much from this device except to read basic ebooks. The  |                                          |\n",
       "|        good thing is it's cheap and good to read in the sun.         |                                          |\n",
       "+-----------------------------------------------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">FULL Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "FULL Dataset: \u001b[1m(\u001b[0m\u001b[1;36m500\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m400\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m100\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:85</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                           \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m85\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  30   | tensor(0.1029, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  30   | tensor(0.1029, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Training Status                       </span>\n",
       "+----------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                    Loss                   </span>|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  30   | tensor(0.1029, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "|  2   |  40   | tensor(0.1207, grad_fn=&lt;NllLossBackward0&gt;)|\n",
       "+----------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Training Status                       \u001b[0m\n",
       "+----------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                   Loss                   \u001b[0m|\n",
       "|------+-------+-------------------------------------------|\n",
       "|  0   |   0   | tensor(8.6769, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  10   | tensor(2.1023, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  20   | tensor(1.0580, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  30   | tensor(0.4254, grad_fn=<NllLossBackward0>)|\n",
       "|  0   |  40   | tensor(0.5253, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |   0   | tensor(0.3429, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  10   | tensor(0.4106, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  20   | tensor(0.3687, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  30   | tensor(0.2390, grad_fn=<NllLossBackward0>)|\n",
       "|  1   |  40   | tensor(0.3186, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |   0   | tensor(0.2491, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  10   | tensor(0.3543, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  20   | tensor(0.1291, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  30   | tensor(0.1029, grad_fn=<NllLossBackward0>)|\n",
       "|  2   |  40   | tensor(0.1207, grad_fn=<NllLossBackward0>)|\n",
       "+----------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:20:51] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:90</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:20:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                     \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m90\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:20:53] </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:97</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:20:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                            \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m97\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                      \u001b[2m                                  \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Completed \u001b[1;36m10\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:27:53] </span><span style=\"font-weight: bold\">[</span>Validation Completed.<span style=\"font-weight: bold\">]</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-27-7a5edd4898d1&gt;:105</span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[20:27:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mValidation Completed.\u001b[1m]\u001b[0m                                              \u001b[2m<ipython-input-27-7a5edd4898d1>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m105\u001b[0m\n",
       "\u001b[2;36m           \u001b[0m                                                                     \u001b[2m                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span> Model saved @ review_outputs\\review_model_files\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m Model saved @ review_outputs\\review_model_files\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Validation<span style=\"font-weight: bold\">]</span> Generation on Validation data saved @ review_outputs\\review_predictions.csv\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mValidation\u001b[1m]\u001b[0m Generation on Validation data saved @ review_outputs\\review_predictions.csv\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ review_outputs\\review_logs.txt\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ review_outputs\\review_logs.txt\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# T5 accepts prefix of the task to be performed:\n",
    "# Since we are summarizing, let's add summarize to source text as a prefix\n",
    "df[\"Review\"] = \"Extract Product: \" + df[\"Review\"]\n",
    "\n",
    "T5Trainer(\n",
    "    dataframe=df[:500], #INPUT FIRST 500 ROWS ONLY.\n",
    "    source_text=\"Review\",\n",
    "    target_text=\"Product\",\n",
    "    model_params=model_params,\n",
    "    output_dir=\"review_outputs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6fa204cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b29836e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':Positive :Unknown, Extract :Positive']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"./review_outputs/model_files\",local_files_only=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./review_outputs/model_files\",local_files_only=True)\n",
    "\n",
    "ids = tokenizer(\"Extract Product :This laptop has good battery life. \\\n",
    "It works quite fast i must say\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "print(preds)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b40000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = tokenizer(\"translate from English to Hindi: The house is wonderful. It is painted blue\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
